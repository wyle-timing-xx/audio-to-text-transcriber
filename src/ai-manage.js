// src/index.js
import { createClient, LiveTranscriptionEvents } from '@deepgram/sdk';
import { spawn } from 'child_process';
import { createWriteStream, mkdirSync, appendFileSync } from 'fs';
import { dirname } from 'path';
import dotenv from 'dotenv';
import fetch from 'node-fetch'; // npm i node-fetch@2 (or use global fetch in newer Node)
import { pipeline } from 'stream';
import { promisify } from 'util';

// åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config();

// é…ç½®é¡¹
const CONFIG = {
  deepgramApiKey: process.env.DEEPGRAM_API_KEY,
  audioDevice: process.env.AUDIO_DEVICE || ':1',
  language: process.env.LANGUAGE || 'zh',
  model: process.env.MODEL || 'nova-2',
  smartFormat: process.env.SMART_FORMAT === 'true',
  punctuate: process.env.PUNCTUATE === 'true',
  outputFile: process.env.OUTPUT_FILE || 'transcripts/output.txt',
  qaOutputFile: process.env.QA_OUTPUT_FILE || 'transcripts/qa_output.txt',
  saveToFile: process.env.SAVE_TO_FILE !== 'false',
  logToConsole: process.env.LOG_TO_CONSOLE !== 'false',
  // AI é…ç½®
  aiProvider: (process.env.AI_PROVIDER || 'openai').toLowerCase(), // openai | claude | deepseek
  openaiApiKey: process.env.OPENAI_API_KEY,
  claudeApiKey: process.env.CLAUDE_API_KEY,
  deepseekApiKey: process.env.DEEPSEEK_API_KEY,
  // prompt / behavior
  aiSystemPrompt: process.env.AI_SYSTEM_PROMPT || `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚å½“å‰å¯¹è¯ä¸ºâ€œè¯­éŸ³é—®ç­”â€ã€‚è¦æ±‚ï¼š
1) è¿™æ˜¯ç”¨æˆ·è¯´å‡ºçš„è¯­éŸ³è½¬ä¸ºæ–‡å­—åçš„å†…å®¹ï¼Œåˆ¤å®šç”¨æˆ·æ˜¯å¦å·²ç»é—®å®Œï¼ˆå¯ä¾æ®åœé¡¿/æ ‡ç‚¹ï¼‰ï¼Œå¦‚æœæœªé—®å®Œè¯·ç­‰å¾…æ›´å¤šè¾“å…¥ï¼›å¦‚æœå·²é—®å®Œè¯·ç›´æ¥ä»¥å›ç­”è€…è§’è‰²ç»™å‡ºå›ç­”ã€‚
2) å›ç­”è¦ç®€æ´ã€å‡†ç¡®ï¼Œå¿…è¦æ—¶ç»™å‡ºæ­¥éª¤/æç¤ºã€‚
3) å¦‚æœç”¨æˆ·æœ‰åç»­é—®é¢˜ï¼Œè¯·åœ¨ç»“å°¾æç¤ºç”¨æˆ·å¯ä»¥ç»§ç»­è¿½é—®ã€‚
`,
  // é™é»˜æ£€æµ‹ï¼ˆæ¯«ç§’ï¼‰ â€” åœ¨æ— æ–°è½¬å½•ç‰‡æ®µçš„æƒ…å†µä¸‹åˆ¤å®šç”¨æˆ·å·²ç»“æŸä¸€å¥è¯
  silenceTimeoutMs: parseInt(process.env.SILENCE_TIMEOUT_MS || '1500', 10),
  // éƒ¨åˆ†ä¸ŠæŠ¥ç­–ç•¥ï¼šæ¯å½“æ¥æ”¶åˆ°ä¸€ä¸ª transcript chunk å°±å‘é€åˆ° AI çš„â€œè®°å½•â€æ¥å£ï¼›æœ€ç»ˆåœ¨ silenceTimeout è§¦å‘å®Œæ•´æé—®
  partialSend: process.env.PARTIAL_SEND !== 'false'
};

// éªŒè¯å¿…éœ€é…ç½®
if (!CONFIG.deepgramApiKey) {
  console.error('âŒ Error: DEEPGRAM_API_KEY is not set in .env file');
  process.exit(1);
}

if (CONFIG.aiProvider === 'openai' && !CONFIG.openaiApiKey) {
  console.error('âŒ Error: OPENAI_API_KEY required for OpenAI provider');
  process.exit(1);
}
if (CONFIG.aiProvider === 'claude' && !CONFIG.claudeApiKey) {
  console.error('âŒ Error: CLAUDE_API_KEY required for Claude provider');
  process.exit(1);
}
if (CONFIG.aiProvider === 'deepseek' && !CONFIG.deepseekApiKey) {
  console.error('âŒ Error: DEEPSEEK_API_KEY required for Deepseek provider');
  process.exit(1);
}

// åˆ›å»ºè¾“å‡ºç›®å½•
if (CONFIG.saveToFile) {
  mkdirSync(dirname(CONFIG.outputFile), { recursive: true });
  mkdirSync(dirname(CONFIG.qaOutputFile), { recursive: true });
}

class AIManager {
  constructor(config) {
    this.config = config;
    this.provider = config.aiProvider;
    this.buffer = ''; // å½“å‰é—®é¢˜ç¼“å†²ï¼ˆå¢é‡æ‹¼æ¥ï¼‰
    this.conversationHistory = []; // [{role, content, timestamp}]
    this.silenceTimer = null;
    this.isProcessing = false; // æ˜¯å¦æ­£åœ¨ç­‰å¾… AI æœ€ç»ˆå›ç­”
  }

  // å°† fragment æ·»åŠ åˆ° bufferï¼Œå¹¶ï¼ˆå¯é€‰ï¼‰åš partial sendï¼ˆè®°å½•/ä¸Šä¸‹æ–‡ï¼‰
  async pushTranscriptFragment(fragment) {
    const text = fragment.trim();
    if (!text) return;
    const ts = new Date().toISOString();
    this.buffer += (this.buffer ? ' ' : '') + text;

    // è®°å½•å¢é‡åˆ°ä¼šè¯å†å²ï¼ˆä½†æ ‡æ³¨ä¸º partialï¼‰
    this.conversationHistory.push({ role: 'user_partial', content: text, timestamp: ts });

    if (this.config.partialSend) {
      // è½»é‡åŒ–ä¸ŠæŠ¥ï¼šå¯é€‰æ‹©æŠŠ partial å‘é€ç»™ AI åšä¸Šä¸‹æ–‡è®°å½•ï¼ˆéè¯·æ±‚ç­”æ¡ˆï¼‰
      // æˆ‘ä»¬å®ç°ä¸ºä¸€ä¸ª "note" call to provider â€” provider å¯ä»¥å¿½ç•¥æˆ–è®°å½•
      try {
        await this._notifyProviderOfPartial(text);
      } catch (e) {
        // ä¸é˜»å¡ä¸»æµç¨‹
        console.error('âš ï¸ Partial send failed:', e.message || e);
      }
    }

    // é‡ç½®é™é»˜è®¡æ—¶å™¨
    this._resetSilenceTimer();
  }

  _resetSilenceTimer() {
    if (this.silenceTimer) clearTimeout(this.silenceTimer);
    this.silenceTimer = setTimeout(() => this._onSilenceTimeout(), this.config.silenceTimeoutMs);
  }

  async _onSilenceTimeout() {
    // è¶…è¿‡é™é»˜é˜ˆå€¼ï¼Œè®¤å®šä¸€å¥â€œç”¨æˆ·è¯â€ç»“æŸ â†’ è§¦å‘æœ€ç»ˆé—®ç­”
    if (!this.buffer || this.isProcessing) {
      this.buffer = '';
      return;
    }
    const question = this.buffer.trim();
    this.buffer = '';
    // add final user message
    this.conversationHistory.push({ role: 'user', content: question, timestamp: new Date().toISOString() });
    // call AI for answer
    try {
      this.isProcessing = true;
      await this.getAnswerForQuestion(question);
    } finally {
      this.isProcessing = false;
    }
  }

  // å°†éƒ¨åˆ†ç‰‡æ®µé€šçŸ¥ providerï¼ˆéå¼ºåˆ¶ï¼‰
  async _notifyProviderOfPartial(text) {
    // For simplicity we call provider with a "log" endpoint if available.
    // Implementations can be no-op for providers that don't support it.
    if (this.provider === 'openai') {
      // noop (we rely on final call)
      return;
    } else if (this.provider === 'claude') {
      return;
    } else if (this.provider === 'deepseek') {
      // noop
      return;
    }
  }

  // è§¦å‘è¯·æ±‚ AI è·å–ç­”æ¡ˆï¼ˆæœ€ç»ˆå›ç­”ï¼‰ï¼Œå¹¶æµå¼å°†ç­”æ¡ˆè¾“å‡ºåˆ°æ§åˆ¶å° + æ–‡ä»¶
  async getAnswerForQuestion(question) {
    const startTs = new Date().toISOString();
    const systemPrompt = this.config.aiSystemPrompt;

    // Build messages (conversation history + current question)
    const messages = [
      { role: 'system', content: systemPrompt },
      // include last N user messages for context (å¯æ”¹)
    ];
    // include some recent history
    const recent = this.conversationHistory.slice(-10).map(h => {
      // map partial -> user
      const role = (h.role === 'user' || h.role === 'user_partial') ? 'user' : h.role;
      return { role, content: h.content };
    });
    messages.push(...recent);
    messages.push({ role: 'user', content: question });

    // Save QA header in file
    const qaHeader = `\n\n=== QA Session Started: ${startTs} (provider=${this.provider}) ===\nQ: ${question}\n`;
    if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, qaHeader);

    // Dispatch to provider
    if (this.provider === 'openai') {
      await this._callOpenAIStream(messages);
    } else if (this.provider === 'claude') {
      await this._callClaudeStream(messages);
    } else if (this.provider === 'deepseek') {
      await this._callDeepseek(messages);
    } else {
      console.warn('âš ï¸ Unknown AI provider:', this.provider);
    }

    const endTs = new Date().toISOString();
    if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, `\n=== QA Session Ended: ${endTs} ===\n`);
  }

  // OpenAI streaming implementation (v1 chat completions stream)
  async _callOpenAIStream(messages) {
    const apiKey = this.config.openaiApiKey;
    const url = 'https://api.openai.com/v1/chat/completions';

    // We will request stream=true and parse the SSE-like stream
    const body = {
      model: 'gpt-4o-mini', // or another model; could be env-configurable
      messages: messages,
      temperature: 0.2,
      stream: true
    };

    const res = await fetch(url, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(body)
    });

    if (!res.ok) {
      const text = await res.text();
      console.error('âŒ OpenAI error:', res.status, text);
      if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, `OpenAI error: ${res.status}\n${text}\n`);
      return;
    }

    // æµå¼è§£æ
    const reader = res.body.getReader();
    const decoder = new TextDecoder('utf-8');
    let done = false;
    let partialAnswer = '';

    while (!done) {
      const { value, done: readerDone } = await reader.read();
      done = readerDone;
      if (value) {
        const chunk = decoder.decode(value, { stream: true });
        // OpenAI stream uses lines starting with "data: "
        const lines = chunk.split(/\r?\n/).filter(l => l.trim().length > 0);
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const payload = line.replace(/^data: /, '');
            if (payload === '[DONE]') {
              // finished
              if (this.config.logToConsole) console.log('\n--- OpenAI stream done ---\n');
              break;
            }
            try {
              const parsed = JSON.parse(payload);
              const delta = parsed.choices?.[0]?.delta?.content || parsed.choices?.[0]?.text;
              if (delta) {
                process.stdout.write(delta);
                partialAnswer += delta;
                if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, delta);
              }
            } catch (e) {
              // ignore JSON parse errors
            }
          } else {
            // non-data lines (ignore)
          }
        }
      }
    }

    // push assistant record to conversationHistory
    this.conversationHistory.push({ role: 'assistant', content: partialAnswer, timestamp: new Date().toISOString() });
    if (this.config.logToConsole) console.log('\n'); // newline after stream
  }

  // Claude streaming (Anthropic) - pseudo-implementation using their streaming API format
  async _callClaudeStream(messages) {
    // Anthropic expects single prompt string. We'll concat messages into prompt.
    const apiKey = this.config.claudeApiKey;
    // Build prompt text
    const promptParts = messages.map(m => {
      const role = m.role === 'system' ? 'System' : (m.role === 'user' ? 'User' : 'Assistant');
      return `${role}: ${m.content}`;
    });
    const prompt = promptParts.join('\n') + '\nAssistant:';

    const url = 'https://api.anthropic.com/v1/complete'; // check Anthropic docs in your environment
    const body = {
      model: 'claude-2.1', // or env-config
      prompt,
      stream: true,
      max_tokens: 800,
      temperature: 0.2
    };

    const res = await fetch(url, {
      method: 'POST',
      headers: {
        'x-api-key': apiKey,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(body)
    });

    if (!res.ok) {
      const text = await res.text();
      console.error('âŒ Claude error:', res.status, text);
      if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, `Claude error: ${res.status}\n${text}\n`);
      return;
    }

    // è§£æ streaming bodyï¼ˆç±»ä¼¼äº OpenAI çš„ streamï¼‰
    const reader = res.body.getReader();
    const decoder = new TextDecoder('utf-8');
    let partialAnswer = '';
    let done = false;

    while (!done) {
      const { value, done: readerDone } = await reader.read();
      done = readerDone;
      if (value) {
        const chunk = decoder.decode(value, { stream: true });
        // Anthropic stream format may differ; common approachï¼šæ¯ä¸ª chunk æ˜¯ JSON è¡Œ
        const lines = chunk.split(/\r?\n/).filter(l => l.trim().length > 0);
        for (const line of lines) {
          try {
            const parsed = JSON.parse(line);
            const token = parsed?.completion;
            if (token) {
              process.stdout.write(token);
              partialAnswer += token;
              if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, token);
            }
          } catch (e) {
            // fallback: treat raw chunk as text
            process.stdout.write(line);
            partialAnswer += line;
            if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, line);
          }
        }
      }
    }

    this.conversationHistory.push({ role: 'assistant', content: partialAnswer, timestamp: new Date().toISOString() });
    if (this.config.logToConsole) console.log('\n');
  }

  // Deepseek (generic HTTP) - no streaming assumed (one-shot)
  async _callDeepseek(messages) {
    const apiKey = this.config.deepseekApiKey;
    const url = process.env.DEEPSEEK_ENDPOINT || 'https://api.deepseek.example.com/v1/qa'; // ç”¨æˆ·éœ€é…ç½®çœŸå® endpoint
    // combine into one question body
    const question = messages.filter(m => m.role === 'user').map(m => m.content).join('\n');
    const body = { question, system: this.config.aiSystemPrompt, max_tokens: 800 };

    const res = await fetch(url, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(body)
    });

    if (!res.ok) {
      const text = await res.text();
      console.error('âŒ Deepseek error:', res.status, text);
      if (this.config.saveToFile) appendFileSync(this.config.qaOutputFile, `Deepseek error: ${res.status}\n${text}\n`);
      return;
    }
    const data = await res.json();
    const answer = data.answer || data.text || JSON.stringify(data);
    // è¾“å‡ºä¸€æ¬¡æ€§ç­”æ¡ˆ
    if (this.config.logToConsole) {
      console.log(answer);
    }
    if (this.config.saveToFile) {
      appendFileSync(this.config.qaOutputFile, answer + '\n');
    }
    this.conversationHistory.push({ role: 'assistant', content: answer, timestamp: new Date().toISOString() });
  }
}

class AudioTranscriber {
  constructor(config) {
    this.config = config;
    this.deepgramClient = null;
    this.deepgramConnection = null;
    this.ffmpegProcess = null;
    this.fileStream = null;
    this.isRunning = false;
    this.aiManager = new AIManager(config);
  }

  initDeepgram() {
    this.deepgramClient = createClient(this.config.deepgramApiKey);
    console.log('âœ… Deepgram client initialized');
  }

  createDeepgramConnection() {
    this.deepgramConnection = this.deepgramClient.listen.live({
      language: this.config.language,
      model: this.config.model,
      smart_format: this.config.smartFormat,
      punctuate: this.config.punctuate,
      encoding: 'linear16',
      sample_rate: 16000,
      channels: 1
    });

    this.deepgramConnection.on(LiveTranscriptionEvents.Open, () => {
      console.log('âœ… Deepgram connection opened');
      console.log('ğŸ™ï¸  Listening to audio...');
    });

    this.deepgramConnection.on(LiveTranscriptionEvents.Transcript, async (data) => {
      const transcript = data.channel.alternatives[0].transcript;
      if (transcript && transcript.trim().length > 0) {
        const timestamp = new Date().toISOString();
        const output = `[${timestamp}] ${transcript}\n`;

        // è¾“å‡ºåˆ°æ§åˆ¶å°
        if (this.config.logToConsole) {
          console.log(`${transcript}`);
        }

        // ä¿å­˜åˆ°æ–‡ä»¶
        if (this.config.saveToFile && this.fileStream) {
          this.fileStream.write(output);
        }

        // æŠŠæ–‡æœ¬ç‰‡æ®µæ¨ç»™ AI managerï¼ˆæµå¼/å¢é‡ï¼‰
        try {
          await this.aiManager.pushTranscriptFragment(transcript);
        } catch (e) {
          console.error('âš ï¸ AI push error:', e.message || e);
        }
      }
    });

    this.deepgramConnection.on(LiveTranscriptionEvents.Metadata, (data) => {
      console.log('ğŸ“Š Metadata:', data);
    });

    this.deepgramConnection.on(LiveTranscriptionEvents.Error, (error) => {
      console.error('âŒ Deepgram error:', error);
    });

    this.deepgramConnection.on(LiveTranscriptionEvents.Close, () => {
      console.log('âš ï¸  Deepgram connection closed');
    });

    console.log('âœ… Deepgram connection created');
  }

  startFFmpegCapture() {
    const ffmpegArgs = [
      '-f', 'avfoundation',
      '-i', `:${this.config.audioDevice}`,
      '-acodec', 'pcm_s16le',
      '-ar', '16000',
      '-ac', '1',
      '-f', 's16le',
      '-'
    ];

    console.log('ğŸš€ Starting FFmpeg audio capture...');
    console.log(`ğŸ“¡ Audio device: ${this.config.audioDevice}`);

    this.ffmpegProcess = spawn('ffmpeg', ffmpegArgs);

    this.ffmpegProcess.stdout.on('data', (audioData) => {
      if (this.deepgramConnection && this.isRunning) {
        this.deepgramConnection.send(audioData);
      }
    });

    this.ffmpegProcess.stderr.on('data', (data) => {
      const message = data.toString();
      if (message.includes('Error') || message.includes('error')) {
        console.error('âš ï¸  FFmpeg:', message);
      }
    });

    this.ffmpegProcess.on('exit', (code) => {
      console.log(`âš ï¸  FFmpeg process exited with code ${code}`);
      this.stop();
    });

    console.log('âœ… FFmpeg capture started');
  }

  createFileStream() {
    if (this.config.saveToFile) {
      this.fileStream = createWriteStream(this.config.outputFile, { flags: 'a' });
      const timestamp = new Date().toISOString();
      this.fileStream.write(`\n\n=== Transcription Session Started: ${timestamp} ===\n\n`);
      console.log(`âœ… Saving transcripts to: ${this.config.outputFile}`);
    }
  }

  async start() {
    if (this.isRunning) {
      console.log('âš ï¸  Transcription is already running');
      return;
    }

    console.log('\nğŸ¯ Starting Audio to Text Transcriber...');
    console.log('='.repeat(50));

    try {
      this.isRunning = true;

      // åˆå§‹åŒ–ç»„ä»¶
      this.initDeepgram();
      this.createDeepgramConnection();
      this.createFileStream();
      this.startFFmpegCapture();

      console.log('='.repeat(50));
      console.log('âœ… Transcription service started successfully!');
      console.log('Press Ctrl+C to stop\n');

    } catch (error) {
      console.error('âŒ Failed to start transcription service:', error);
      this.stop();
    }
  }

  stop() {
    if (!this.isRunning) return;

    console.log('\nâ¹ï¸  Stopping transcription service...');
    this.isRunning = false;

    if (this.ffmpegProcess) {
      this.ffmpegProcess.kill('SIGTERM');
      this.ffmpegProcess = null;
      console.log('âœ… FFmpeg process stopped');
    }

    if (this.deepgramConnection) {
      // finish() might be async; call safely
      try {
        this.deepgramConnection.finish();
      } catch (e) {}
      this.deepgramConnection = null;
      console.log('âœ… Deepgram connection closed');
    }

    if (this.fileStream) {
      const timestamp = new Date().toISOString();
      this.fileStream.write(`\n=== Transcription Session Ended: ${timestamp} ===\n`);
      this.fileStream.end();
      this.fileStream = null;
      console.log('âœ… Output file closed');
    }

    console.log('ğŸ‘‹ Transcription service stopped\n');
  }
}

// åˆ›å»ºè½¬å½•å™¨å®ä¾‹
const transcriber = new AudioTranscriber(CONFIG);

// ä¼˜é›…é€€å‡ºå¤„ç†
process.on('SIGINT', () => {
  console.log('\n\nâš ï¸  Received interrupt signal...');
  transcriber.stop();
  process.exit(0);
});

process.on('SIGTERM', () => {
  transcriber.stop();
  process.exit(0);
});

// å¯åŠ¨æœåŠ¡
transcriber.start();
